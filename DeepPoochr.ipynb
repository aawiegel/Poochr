{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymongo\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-10-27 14:14:11--  https://s3.amazonaws.com/mordecai-geo/GoogleNews-vectors-negative300.bin.gz\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.161.157\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.161.157|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1647046227 (1.5G) [application/octet-stream]\n",
      "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
      "\n",
      "GoogleNews-vectors- 100%[===================>]   1.53G  1.52MB/s    in 13m 25s \n",
      "\n",
      "2017-10-27 14:27:37 (1.95 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get pretrained vectors for word2vec\n",
    "\n",
    "!wget https://s3.amazonaws.com/mordecai-geo/GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gnews_w2v = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431607246399),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321243286133),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.5181134343147278),\n",
       " ('sultan', 0.5098593235015869),\n",
       " ('monarchy', 0.5087411999702454)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnews_w2v.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apartment_complex', 0.7893964052200317),\n",
       " ('townhouse', 0.7727404236793518),\n",
       " ('apartments', 0.7152601480484009),\n",
       " ('bedroom', 0.7045778632164001),\n",
       " ('Apartments', 0.6661036014556885),\n",
       " ('house', 0.6628996133804321),\n",
       " ('duplex', 0.6575543880462646),\n",
       " ('rooming_house', 0.6564425230026245),\n",
       " ('townhome', 0.6522176265716553),\n",
       " ('fourplex', 0.648613691329956)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnews_w2v.most_similar(positive=['apartment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.49586788]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(gnews_w2v['big'].reshape(1, -1), gnews_w2v['small'].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained GloVe model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_w2v = gensim.models.KeyedVectors.load_word2vec_format('./data/glove_w2v_100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.71795845]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(wiki_w2v['small'].reshape(1, -1), wiki_w2v['big'].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30816999,  0.30937999,  0.52802998, -0.92543   , -0.73671001,\n",
       "        0.63475001,  0.44196999,  0.10262   , -0.09142   , -0.56607002,\n",
       "       -0.5327    ,  0.2013    ,  0.77039999, -0.13982999,  0.13727   ,\n",
       "        1.1128    ,  0.89301002, -0.17869   , -0.0019722 ,  0.57288998,\n",
       "        0.59478998,  0.50427997, -0.28990999, -1.34909999,  0.42756   ,\n",
       "        1.27479994, -1.16129994, -0.41084   ,  0.042804  ,  0.54865998,\n",
       "        0.18897   ,  0.3759    ,  0.58034998,  0.66974998,  0.81155998,\n",
       "        0.93864   , -0.51005   , -0.070079  ,  0.82819003, -0.35346001,\n",
       "        0.21086   , -0.24412   , -0.16553999, -0.78358001, -0.48482001,\n",
       "        0.38968   , -0.86356002, -0.016391  ,  0.31984001, -0.49246001,\n",
       "       -0.069363  ,  0.018869  , -0.098286  ,  1.31260002, -0.12116   ,\n",
       "       -1.23989999, -0.091429  ,  0.35293999,  0.64644998,  0.089642  ,\n",
       "        0.70293999,  1.12440002,  0.38639   ,  0.52083999,  0.98786998,\n",
       "        0.79952002, -0.34625   ,  0.14094999,  0.80167001,  0.20987   ,\n",
       "       -0.86006999, -0.15308   ,  0.074523  ,  0.40816   ,  0.019208  ,\n",
       "        0.51586998, -0.34428   , -0.24525   , -0.77983999,  0.27425   ,\n",
       "        0.22418   ,  0.20163999,  0.017431  , -0.014697  , -1.02349997,\n",
       "       -0.39695001, -0.0056188 ,  0.30568999,  0.31748   ,  0.021404  ,\n",
       "        0.11837   , -0.11319   ,  0.42456001,  0.53404999, -0.16717   ,\n",
       "       -0.27184999, -0.62550002,  0.12883   ,  0.62528998, -0.52086002], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_w2v['dog']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dog vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_text_from_num(dogtime_html, synonyms, antonyms):\n",
    "    \"\"\"\n",
    "    Given a BeautifulSoup object dogtime_html generated from the dogtime website,\n",
    "    generate text from numeric features using synonym and antonym dictionaries\n",
    "    \"\"\"\n",
    "    dog_text = ''\n",
    "    \n",
    "    char_dict = dict()\n",
    "    for characteristic in dogtime_html.find_all(class_=\"characteristic item-trigger-title\"):\n",
    "        char_dict[characteristic.text.strip()] =\\\n",
    "                int(characteristic.find_next().find_next()['class'][1].split('-')[-1])\n",
    "    \n",
    "    for trait, value in char_dict.items():\n",
    "        if value > 3:\n",
    "            factor = value - 3\n",
    "\n",
    "            dog_text += factor*(synonyms[trait])\n",
    "            \n",
    "        elif value < 3:\n",
    "            factor = 3 - value\n",
    "            \n",
    "            dog_text += factor*(antonyms[trait])\n",
    "    \n",
    "    return dog_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(\"mongodb://54.67.82.182/dogbreeds\")\n",
    "db = client.dogbreeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trait_synonyms = dict()\n",
    "trait_synonyms['Adaptability'] = 'adaptable '\n",
    "trait_synonyms['Adapts Well to Apartment Living'] = 'apartment '\n",
    "trait_synonyms['Affectionate with Family'] = 'cuddly '\n",
    "trait_synonyms['All Around Friendliness'] = 'friendly '\n",
    "trait_synonyms['Amount Of Shedding'] = ''\n",
    "trait_synonyms['Dog Friendly'] = 'dogs '\n",
    "trait_synonyms['Drooling Potential'] = ''\n",
    "trait_synonyms['Easy To Groom'] = 'grooming '\n",
    "trait_synonyms['Easy To Train'] = 'trainable '\n",
    "trait_synonyms['Energy Level'] = 'energetic '\n",
    "trait_synonyms['Exercise Needs'] = 'active '\n",
    "trait_synonyms['Friendly Toward Strangers'] = 'friendly '\n",
    "trait_synonyms['General Health'] = 'healthy '\n",
    "trait_synonyms['Good For Novice Owners'] = 'novice '\n",
    "trait_synonyms['Health Grooming'] = 'healthy '\n",
    "trait_synonyms['Incredibly Kid Friendly Dogs'] = 'children '\n",
    "trait_synonyms['Intelligence'] = 'intelligent '\n",
    "trait_synonyms['Intensity'] = ''\n",
    "trait_synonyms['Potential For Mouthiness'] = 'fetch '\n",
    "trait_synonyms['Potential For Playfulness'] = 'playful '\n",
    "trait_synonyms['Potential For Weight Gain'] = ''\n",
    "trait_synonyms['Prey Drive'] = 'hunting '\n",
    "trait_synonyms['Sensitivity Level'] = ''\n",
    "trait_synonyms['Size'] = 'big '\n",
    "trait_synonyms['Tendency To Bark Or Howl'] = ''\n",
    "trait_synonyms['Tolerates Being Alone'] = 'alone '\n",
    "trait_synonyms['Tolerates Cold Weather'] = 'cold '\n",
    "trait_synonyms['Tolerates Hot Weather'] = 'hot '\n",
    "trait_synonyms['Trainability'] = 'trainable '\n",
    "trait_synonyms['Wanderlust Potential'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trait_antonyms = dict()\n",
    "trait_antonyms['Adaptability'] = ''\n",
    "trait_antonyms['Adapts Well to Apartment Living'] = 'yard '\n",
    "trait_antonyms['Affectionate with Family'] = ''\n",
    "trait_antonyms['All Around Friendliness'] = ''\n",
    "trait_antonyms['Amount Of Shedding'] = 'clean '\n",
    "trait_antonyms['Dog Friendly'] = 'protective '\n",
    "trait_antonyms['Drooling Potential'] = 'clean '\n",
    "trait_antonyms['Easy To Groom'] = ''\n",
    "trait_antonyms['Easy To Train'] = ''\n",
    "trait_antonyms['Energy Level'] = 'calm '\n",
    "trait_antonyms['Exercise Needs'] = 'lazy '\n",
    "trait_antonyms['Friendly Toward Strangers'] = 'security '\n",
    "trait_antonyms['General Health'] = ''\n",
    "trait_antonyms['Good For Novice Owners'] = ''\n",
    "trait_antonyms['Health Grooming'] = ''\n",
    "trait_antonyms['Incredibly Kid Friendly Dogs'] = ''\n",
    "trait_antonyms['Intelligence'] = ''\n",
    "trait_antonyms['Intensity'] = 'relaxed '\n",
    "trait_antonyms['Potential For Mouthiness'] = 'safe '\n",
    "trait_antonyms['Potential For Playfulness'] = 'aloof '\n",
    "trait_antonyms['Potential For Weight Gain'] = 'thin '\n",
    "trait_antonyms['Prey Drive'] = ''\n",
    "trait_antonyms['Sensitivity Level'] = 'adaptable '\n",
    "trait_antonyms['Size'] = 'small '\n",
    "trait_antonyms['Tendency To Bark Or Howl'] = 'quiet '\n",
    "trait_antonyms['Tolerates Being Alone'] = ''\n",
    "trait_antonyms['Tolerates Cold Weather'] = ''\n",
    "trait_antonyms['Tolerates Hot Weather'] = ''\n",
    "trait_antonyms['Trainability'] = ''\n",
    "trait_antonyms['Wanderlust Potential'] = 'homebody '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.path.curdir, \"data\")\n",
    "image_dir = os.path.join(data_dir, 'Images', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dog_dirs = [direct for direct in os.listdir(image_dir)\\\n",
    "            if os.path.isdir(os.path.join(image_dir, direct))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dog_dirs.remove('not_dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dog_breeds = [dog.split('-', 1)[1].lower() for dog in dog_dirs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "breed_text = dict()\n",
    "for breed, dog_dir in zip(dog_breeds, dog_dirs):\n",
    "    dog_content = db.dogbreeds.find_one({\"breed\" : breed})\n",
    "    dogtime_html = BeautifulSoup(dog_content[\"dogtime_content\"], \"lxml\")\n",
    "    breed_text[dog_dir] = generate_text_from_num(dogtime_html, trait_synonyms, trait_antonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "breed_vecs = dict()\n",
    "for breed, text in breed_text.items():\n",
    "    trait_list = text.split(' ')[:-2]\n",
    "    breed_vecs[breed] = np.zeros_like(wiki_w2v['dog'])\n",
    "    for trait in trait_list:\n",
    "        breed_vecs[breed] += wiki_w2v[trait]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5506736]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(breed_vecs['n02094433-Yorkshire_terrier'].reshape(1, -1), wiki_w2v['apartment'].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "breed_mat = np.zeros((115, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "breed_vecs['not_dog'] = np.zeros_like(breed_vecs['n02085620-Chihuahua'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "breeds = np.load('breed_indices.npy').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for breed, index in breeds.items():\n",
    "    breed_mat[index] += breed_vecs[breed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.01540565e+00,   5.45926809e+00,   3.44888043e+00, ...,\n",
       "         -3.10276031e+00,   3.75666428e+00,   7.80975819e-03],\n",
       "       [ -1.77221918e+00,   1.87089062e+00,   4.58829021e+00, ...,\n",
       "         -1.02567804e+00,  -1.15763593e+00,   1.68762589e+00],\n",
       "       [ -2.43179893e+00,   6.07515049e+00,   5.99154949e+00, ...,\n",
       "          1.17739439e-02,   1.68751764e+00,   4.56869990e-01],\n",
       "       ..., \n",
       "       [ -4.00478983e+00,   1.32845678e+01,   4.60762930e+00, ...,\n",
       "         -9.75337982e-01,   2.10804796e+00,  -2.04442978e-01],\n",
       "       [ -6.40680075e-01,   3.68850780e+00,   2.27188468e+00, ...,\n",
       "         -4.15260410e+00,   5.04354334e+00,   6.00267887e-01],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breed_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('breed_glove_matrix.npy', breed_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([114,  28,  76, 107,  32,  48,  98, 105,  85,  35,  19,  95, 109,\n",
       "        97, 103,  81,  33,  11,  26,  29,  78,  68,  12,  87,   9,  86,\n",
       "        70,  31,  75,  61,  91,  50,  38,  45,  49, 113,  54,  60,  22,\n",
       "        21,  51,  18,  56,  94,  58,  71, 102,  62,  92,  64,  66,  25,\n",
       "        14,  42,  47, 108,  90,  96,  73,   7,  46,   1,   8,  16,  63,\n",
       "        88,  36,  57,  74,  89,  82,   3, 104,  55,  10,  84, 100,  37,\n",
       "        30,  17,  83,  99,  40,  24,  44,  79,   0,  13,  80,  43,  41,\n",
       "        77,  23,  69, 110, 106,  59,  15,   4,  52,   5,  34, 111,  39,\n",
       "        27,  72,  20,   6, 112,  67, 101,  65,   2,  93,  53])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(cosine_similarity((wiki_w2v['novice']).reshape(1, -1), breed_mat)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n02085620-Chihuahua': 0,\n",
       " 'n02085782-Japanese_spaniel': 1,\n",
       " 'n02085936-Maltese_dog': 2,\n",
       " 'n02086079-Pekinese': 3,\n",
       " 'n02086240-Shih-Tzu': 4,\n",
       " 'n02086646-Blenheim_spaniel': 5,\n",
       " 'n02086910-papillon': 6,\n",
       " 'n02087046-toy_terrier': 7,\n",
       " 'n02087394-Rhodesian_ridgeback': 8,\n",
       " 'n02088094-Afghan_hound': 9,\n",
       " 'n02088238-basset': 10,\n",
       " 'n02088364-beagle': 11,\n",
       " 'n02088466-bloodhound': 12,\n",
       " 'n02088632-bluetick': 13,\n",
       " 'n02089078-black-and-tan_coonhound': 14,\n",
       " 'n02089867-Walker_hound': 15,\n",
       " 'n02089973-English_foxhound': 16,\n",
       " 'n02090379-redbone': 17,\n",
       " 'n02090622-borzoi': 18,\n",
       " 'n02090721-Irish_wolfhound': 19,\n",
       " 'n02091032-Italian_greyhound': 20,\n",
       " 'n02091134-whippet': 21,\n",
       " 'n02091244-Ibizan_hound': 22,\n",
       " 'n02091467-Norwegian_elkhound': 23,\n",
       " 'n02091635-otterhound': 24,\n",
       " 'n02091831-Saluki': 25,\n",
       " 'n02092002-Scottish_deerhound': 26,\n",
       " 'n02092339-Weimaraner': 27,\n",
       " 'n02093256-Staffordshire_bullterrier': 28,\n",
       " 'n02093428-American_Staffordshire_terrier': 29,\n",
       " 'n02093647-Bedlington_terrier': 30,\n",
       " 'n02093754-Border_terrier': 31,\n",
       " 'n02093859-Kerry_blue_terrier': 32,\n",
       " 'n02093991-Irish_terrier': 33,\n",
       " 'n02094114-Norfolk_terrier': 34,\n",
       " 'n02094258-Norwich_terrier': 35,\n",
       " 'n02094433-Yorkshire_terrier': 36,\n",
       " 'n02095314-wire-haired_fox_terrier': 37,\n",
       " 'n02095570-Lakeland_terrier': 38,\n",
       " 'n02095889-Sealyham_terrier': 39,\n",
       " 'n02096051-Airedale': 40,\n",
       " 'n02096177-cairn': 41,\n",
       " 'n02096294-Australian_terrier': 42,\n",
       " 'n02096437-Dandie_Dinmont': 43,\n",
       " 'n02096585-Boston_bull': 44,\n",
       " 'n02097047-miniature_schnauzer': 45,\n",
       " 'n02097130-giant_schnauzer': 46,\n",
       " 'n02097209-standard_schnauzer': 47,\n",
       " 'n02097298-Scotch_terrier': 48,\n",
       " 'n02097474-Tibetan_terrier': 49,\n",
       " 'n02097658-silky_terrier': 50,\n",
       " 'n02098105-soft-coated_wheaten_terrier': 51,\n",
       " 'n02098286-West_Highland_white_terrier': 52,\n",
       " 'n02098413-Lhasa': 53,\n",
       " 'n02099267-flat-coated_retriever': 54,\n",
       " 'n02099429-curly-coated_retriever': 55,\n",
       " 'n02099601-golden_retriever': 56,\n",
       " 'n02099712-Labrador_retriever': 57,\n",
       " 'n02099849-Chesapeake_Bay_retriever': 58,\n",
       " 'n02100236-German_short-haired_pointer': 59,\n",
       " 'n02100583-vizsla': 60,\n",
       " 'n02100735-English_setter': 61,\n",
       " 'n02100877-Irish_setter': 62,\n",
       " 'n02101006-Gordon_setter': 63,\n",
       " 'n02101388-Brittany_spaniel': 64,\n",
       " 'n02101556-clumber': 65,\n",
       " 'n02102040-English_springer': 66,\n",
       " 'n02102177-Welsh_springer_spaniel': 67,\n",
       " 'n02102318-cocker_spaniel': 68,\n",
       " 'n02102480-Sussex_spaniel': 69,\n",
       " 'n02102973-Irish_water_spaniel': 70,\n",
       " 'n02104029-kuvasz': 71,\n",
       " 'n02104365-schipperke': 72,\n",
       " 'n02105056-groenendael': 73,\n",
       " 'n02105162-malinois': 74,\n",
       " 'n02105251-briard': 75,\n",
       " 'n02105505-komondor': 76,\n",
       " 'n02105641-Old_English_sheepdog': 77,\n",
       " 'n02105855-Shetland_sheepdog': 78,\n",
       " 'n02106030-collie': 79,\n",
       " 'n02106166-Border_collie': 80,\n",
       " 'n02106382-Bouvier_des_Flandres': 81,\n",
       " 'n02106550-Rottweiler': 82,\n",
       " 'n02106662-German_shepherd': 83,\n",
       " 'n02107142-Doberman': 84,\n",
       " 'n02107312-miniature_pinscher': 85,\n",
       " 'n02107574-Greater_Swiss_Mountain_dog': 86,\n",
       " 'n02107683-Bernese_mountain_dog': 87,\n",
       " 'n02107908-Appenzeller': 88,\n",
       " 'n02108000-EntleBucher': 89,\n",
       " 'n02108089-boxer': 90,\n",
       " 'n02108422-bull_mastiff': 91,\n",
       " 'n02108551-Tibetan_mastiff': 92,\n",
       " 'n02108915-French_bulldog': 93,\n",
       " 'n02109047-Great_Dane': 94,\n",
       " 'n02109525-Saint_Bernard': 95,\n",
       " 'n02109961-Eskimo_dog': 96,\n",
       " 'n02110063-malamute': 97,\n",
       " 'n02110185-Siberian_husky': 98,\n",
       " 'n02110627-affenpinscher': 99,\n",
       " 'n02110806-basenji': 100,\n",
       " 'n02110958-pug': 101,\n",
       " 'n02111129-Leonberg': 102,\n",
       " 'n02111277-Newfoundland': 103,\n",
       " 'n02111500-Great_Pyrenees': 104,\n",
       " 'n02111889-Samoyed': 105,\n",
       " 'n02112018-Pomeranian': 106,\n",
       " 'n02112137-chow': 107,\n",
       " 'n02112350-keeshond': 108,\n",
       " 'n02112706-Brabancon_griffon': 109,\n",
       " 'n02113023-Pembroke': 110,\n",
       " 'n02113186-Cardigan': 111,\n",
       " 'n02113799-standard_poodle': 112,\n",
       " 'n02113978-Mexican_hairless': 113,\n",
       " 'not_dog': 114}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test potential user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = \"Hello: this is a sentence with punctuation! Doesn't it look great?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ':',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'sentence',\n",
       " 'with',\n",
       " 'punctuation',\n",
       " '!',\n",
       " 'Does',\n",
       " \"n't\",\n",
       " 'it',\n",
       " 'look',\n",
       " 'great',\n",
       " '?']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'sentence',\n",
       " 'with',\n",
       " 'punctuation',\n",
       " 'Doesn',\n",
       " 't',\n",
       " 'it',\n",
       " 'look',\n",
       " 'great']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.compile('\\w+').findall(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ':',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'sentence',\n",
       " 'with',\n",
       " 'punctuation',\n",
       " '!',\n",
       " 'Doesn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'it',\n",
       " 'look',\n",
       " 'great',\n",
       " '?']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.tokenize.regexp.WordPunctTokenizer().tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71795842658489328"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_w2v.similarity('big', 'small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.57309994e-01,   3.95300001e-01,   6.35860026e-01,\n",
       "        -1.09749997e+00,  -9.57679987e-01,  -1.38410004e-02,\n",
       "        -1.98530003e-01,   2.54180014e-01,   3.67309988e-01,\n",
       "        -1.74860001e-01,   2.76849985e-01,   3.19429994e-01,\n",
       "         3.00779998e-01,   6.85309991e-02,  -1.59170002e-01,\n",
       "        -2.19439998e-01,   6.40970021e-02,   8.47450018e-01,\n",
       "        -6.19889975e-01,   5.41729987e-01,   2.79210001e-01,\n",
       "         5.03830016e-01,   2.14600004e-02,  -2.05709994e-01,\n",
       "         7.79939964e-02,   3.22290003e-01,  -4.91829991e-01,\n",
       "        -1.14110005e+00,   2.33329996e-01,  -5.43579996e-01,\n",
       "         9.22849998e-02,   8.68600011e-01,   6.91270009e-02,\n",
       "         1.92289993e-01,   2.83740014e-01,   4.60139990e-01,\n",
       "        -2.83199996e-01,   4.53839988e-01,   3.52090001e-01,\n",
       "        -4.91730005e-01,  -1.47709996e-01,  -7.17670023e-02,\n",
       "        -2.43550003e-01,  -6.30890012e-01,  -6.77969992e-01,\n",
       "        -1.31640002e-01,   3.59739989e-01,  -7.52919972e-01,\n",
       "         3.82039994e-02,  -1.76950002e+00,   1.88930005e-01,\n",
       "        -1.88720003e-01,  -2.02680007e-01,   8.30900013e-01,\n",
       "         7.78699964e-02,  -2.62129998e+00,   8.19410011e-02,\n",
       "         2.72619992e-01,   1.62160003e+00,   8.61660004e-01,\n",
       "        -2.15820000e-01,   1.00979996e+00,  -7.81220019e-01,\n",
       "        -1.16630003e-01,   1.06289995e+00,   1.58299997e-01,\n",
       "         1.10090005e+00,   7.03239977e-01,  -6.04809999e-01,\n",
       "        -4.59069997e-01,   7.98619986e-02,  -6.17940009e-01,\n",
       "        -9.38960016e-02,  -5.03629982e-01,  -1.22170001e-01,\n",
       "        -1.78569998e-03,  -3.22350003e-02,  -1.05899997e-01,\n",
       "        -6.92319989e-01,   7.64850006e-02,   6.03839993e-01,\n",
       "        -5.60750008e-01,  -9.63720024e-01,  -7.01920018e-02,\n",
       "        -2.07879996e+00,  -5.64230025e-01,   1.75740004e-01,\n",
       "        -2.49610003e-02,  -4.53489989e-01,  -3.92870009e-01,\n",
       "        -8.05730000e-02,  -3.76340002e-01,   3.50829996e-02,\n",
       "        -3.96620005e-01,  -7.61650026e-01,   1.51130006e-01,\n",
       "        -1.30329996e-01,  -2.85129994e-01,   1.98530003e-01,\n",
       "         6.74640000e-01], dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_w2v[\"n't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_input(sentence):\n",
    "    \"\"\"\n",
    "    Tokenizes a sentence, removes punctuation, and converts to lowercase letters.\n",
    "    \"\"\"\n",
    "    l = nltk.word_tokenize(sentence)\n",
    "    return [x.lower() for x in l if not re.fullmatch('[' + string.punctuation + ']+', x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vec = np.zeros(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = tokenize_input(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    word_vec += wiki_w2v[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[114,  63,  82,  84,  59,  68,  56,  65,  29,  93,  90,  67,  40,\n",
       "         79,  80, 106,  57,  66,  17,  52,  13, 110, 109,  70,  37,  46,\n",
       "         83, 108,  60, 113,  55,  50,  10,  28,  54,  32,  30,  96,  27,\n",
       "         74,  64,  62,  45,   6,  23, 101,  76,   2,   5, 112, 105,  11,\n",
       "         97, 111,  16,  73,  42,  14,  34,  61,  58,  51,  98, 103,  44,\n",
       "          3,  99,  15,  94,   1,   0,  41,  22,   7,  21,  35,  33,  20,\n",
       "         81,  89,  85,  12,  78,  69,  88,  77,  31,  87,   8,  49,   4,\n",
       "         86,  75,  39,  43, 100,  48, 104,  18,  95,  25,  38,  47,  91,\n",
       "         24,  72,  53,  19,  26,  36,  92,   9, 107, 102,  71]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(cosine_similarity(word_vec.reshape(1, -1), breed_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tokenize_input(\"A sentence that may contain a word you've not seen before: l'cie.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_words(model, words):\n",
    "    \"\"\"\n",
    "    Take a list of words, and convert it into the sum of the word vectors\n",
    "    for the model, ignoring out of vocabulary words\n",
    "    \"\"\"\n",
    "    word_vec = np.zeros(len(model['you']))\n",
    "    for word in words:\n",
    "        try:\n",
    "            word_vec += wiki_w2v[word]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_desc = \"playful, affectionate children\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_vec = vectorize_words(wiki_w2v, tokenize_input(dog_desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[114, 107,   1,  76,  50,  71,  36,   3,  45,   2,  42,  32, 106,\n",
       "         58,  97,  75,  99,  47, 113,  46,  18,  68,   9,  38,  81,  15,\n",
       "        104, 102,  37,  25,   6,  53,  26,  85,  19,  52,  34,  40,  43,\n",
       "        111,  70,  74,  88,  65,  20,  16,  63,  48,   4,  23,  69, 110,\n",
       "        103,  87,  78,   7,  39,   8,  73, 112, 100,   0,  80,  24,  67,\n",
       "         86,  77,  66,  35,  14,  72, 101,  60,  31,  30,  21,  44,  33,\n",
       "         59, 105,  49,  84,  27,  79,  11,  51,  22,  92,  62,  61,  96,\n",
       "         41,  12,  94,  89,  95,  10,  93, 109,  54,  55,   5,  91,  64,\n",
       "         83,  98, 108,  90,  13,  82,  57,  28,  29,  56,  17]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(cosine_similarity(dog_vec.reshape(1, -1), breed_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
